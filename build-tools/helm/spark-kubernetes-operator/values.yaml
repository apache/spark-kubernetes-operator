# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

image:
  repository: spark-kubernetes-operator
  pullPolicy: IfNotPresent
  tag: 0.1.0-SNAPSHOT
  # If image digest is set then it takes precedence and the image tag will be ignored
  # digest: ""

imagePullSecrets: [ ]

operatorDeployment:
  # Replicas must be 1 unless leader election is enabled
  replicas: 1
  # Strategy type must be 'Recreate' unless leader election is enabled
  strategy:
    type: Recreate
  operatorPod:
    priorityClassName: null
    annotations: { }
    labels: { }
    affinity: { }
    nodeSelector: { }
    # Node tolerations for operator pod assignment
    # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    tolerations: [ ]
    # Topology spread constrains
    # https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/
    topologySpreadConstraints: [ ]
    operatorContainer:
      jvmArgs: "-Dfile.encoding=UTF8"
      env:
      envFrom:
      volumeMounts: { }
      resources:
        limits:
          cpu: "1"
          ephemeral-storage: 2Gi
          memory: 2Gi
        requests:
          cpu: "1"
          ephemeral-storage: 2Gi
          memory: 2Gi
      probes:
        port: 19091
        livenessProbe:
          periodSeconds: 10
          initialDelaySeconds: 30
        startupProbe:
          failureThreshold: 30
          periodSeconds: 10
      # By default, operator container is configured to comply restricted standard
      # https://kubernetes.io/docs/concepts/security/pod-security-standards/
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
        runAsNonRoot: true
        runAsUser: 185
        seccompProfile:
          type: RuntimeDefault
    additionalContainers: { }
    volumes: { }
    securityContext: { }
    dnsPolicy:
    dnsConfig:

operatorRbac:
  serviceAccount:
    create: true
    name: "spark-operator"
  clusterRole:
    create: true
    name: "spark-operator-clusterrole"
  clusterRoleBinding:
    create: true
    name: "spark-operator-clusterrolebinding"
  role:
    # If enabled, a Role would be created inside each app namespace, as configured
    # in {appResources.namespaces.data} for operator. Please enable *at least one* of
    # {operatorRbac.clusterRole.create} or {operatorRbac.role.create} for operator
    # unless permission is provided separately from the chart
    create: false
    name: "spark-operator-role"
  roleBinding:
    # If enabled, a RoleBinding would be created inside each app namespace, as configured
    # in {appResources.namespaces.data} for operator. Please enable *at least one* of
    # {operatorRbac.clusterRoleBinding.create} or {operatorRbac.roleBinding.create} for
    # operator unless permission is provided separately from the chart
    create: false
    name: "spark-operator-rolebinding"
    # It's possible to make the rolebinding refers to role / clusterrole as applicable
    roleRef:
      kind: Role
      name: "spark-operator-role"
  configManagement:
    # Role to enable operator loading hot properties from config map
    create: true
    roleName: "spark-operator-config-monitor"
    roleBindingName: "spark-operator-config-monitor-role-binding"
  labels:
    "app.kubernetes.io/component": "operator-rbac"

appResources:
  # Create namespace(s), service account(s), clusterrole, role(s) and rolebinding(s) for SparkApps
  namespaces:
    create: true
    # When enabled, value for conf property `spark.operator.watched.namespaces` would be set to
    # the values provided in {appResources.namespaces.data} field as well
    overrideWatchedNamespaces: true
    data:
    # - "spark-1"
    # - "spark-2"
  serviceAccount:
    create: true
    name: "spark"
  role:
    # When enabled, a role would be created in each app namespace, as configured in
    # {appResources.namespaces.data} for Spark apps. Please enable *at least one* of
    # {appResources.roles.create} or {appResources.clusterRole.create} for
    # Spark app unless permission is provided separately from the chart
    create: false
    name: "spark-app-role"
  clusterRole:
    # When enabled, a clusterrole would be created for Spark apps. Please enable *at least one* of
    # {appResources.roles.create} or {appResources.clusterRole.create} for Spark app unless
    # permission is provided separately from the chart
    create: true
    name: "spark-app-clusterrole"
  roleBinding:
    create: true
    name: "spark-app-rolebinding"
  sparkApplicationSentinel:
    create: false
    name: "spark-app-sentinel"
    sentinelNamespaces:
      data:
      #  - "spark-1"
      #  - "spark-2"
      # When enabled, a sentinel resources will be deployed to namespace(s) provided/
      # Note that sentinelNamespaces list should be a subset of {appResources.namespaces.data}
      # - "default"
  # App resources are by default annotated to avoid app abort due to operator upgrade
  annotations:
    "helm.sh/resource-policy": keep
  # labels to be added on app resources
  labels:
    "app.kubernetes.io/component": "spark-app"

operatorConfiguration:
  # If set to true, below properties would be appended to default conf files under conf/
  # Otherwise, below would override default conf files
  append: true
  log4j2.properties: |+
    # Logging Overrides
    rootLogger.level=INFO
    rootLogger.appenderRef.stdout.ref = console
    appender.console.type = Console
    appender.console.name = console
    appender.console.target = SYSTEM_ERR
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n%ex
  spark-operator.properties: |+
    # Property Overrides. e.g.
    # spark.kubernetes.operator.reconciler.intervalSeconds=60
  metrics.properties: |+
    # Metrics Properties Overrides
  dynamicConfig:
    # Enable this for hot properties loading.
    enable: false
    # Enable this to create a config map for hot property loading
    create: false
    annotations:
      "helm.sh/resource-policy": keep
    data:
      # Spark Operator Config Runtime Properties Overrides. e.g.
      spark.kubernetes.operator.reconciler.intervalSeconds: "60"
